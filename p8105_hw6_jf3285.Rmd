---
title: "Homework 6"
author: "Jiarui Fu"
date: "11/20/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(modelr)
library(mgcv)
```

## Problem 1
After observing that the 'parity', 'pnumlbw', and 'pnumsga' columns are all zero, these three columns are removed from the dataset. The categorical variables are converted from numeric to factor, recoded and ordered from most frequent to infrequent. First, a linear model with all the variables inside is used to see if there's a good fit, and based on the summary result, non-significant predictors (large p-value) are removed from the model. Then, several attempts are made to find out the interactions between these predictors: baby's head circumference is strongly associated with mother's race and baby's length also depends on the average number of cigarettes mothers smoked per day during pregnancy. 
```{r}
# load and clean the data
birthweight = read.csv("data/birthweight.csv") 
birthweight_clean = birthweight %>%  
  # select important columns only
  select(-parity, -pnumlbw, -pnumsga) %>%
  # as factor(categorical variables)
  mutate(babysex = factor(babysex),
         frace = factor(frace),
         malform = factor(malform),
         mrace = factor(mrace)) %>% 
  # order based on frequency
  mutate(babysex = fct_infreq(babysex),
         frace = fct_infreq(frace),
         malform = fct_infreq(malform),
         mrace = fct_infreq(mrace)) %>% 
  # recode categorical variables
  mutate(babysex = recode(babysex, "1" = "male", "2" = "female"),
         frace = recode(frace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
         malform = recode(malform, "0" = "absent", "1" = "present"),
         mrace = recode(mrace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other"))

# lm using all the variables
fit0 = lm(bwt ~ ., data = birthweight_clean)
summary(fit0)

# select variables of interest
birthweight_fit = birthweight_clean %>% 
  select(babysex, bhead, blength, bwt, gaweeks, mrace, smoken, wtgain) 

# propose a model
fit = lm(bwt ~ babysex + gaweeks + bhead * mrace + blength * smoken + wtgain, data = birthweight_fit)
summary(fit)

# plot of model residuals against fitted values
birthweight_clean %>% 
  modelr::add_residuals(fit) %>% 
  modelr::add_predictions(fit) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point() + ggtitle("Model residuals against fitted values")

model1 = lm(bwt ~ blength + gaweeks, data = birthweight_fit)
summary(model1)
model2 = lm(bwt ~ bhead * blength * babysex, data = birthweight_fit)
summary(model2)

# cross-validated prediction error
cv_df = crossv_mc(birthweight_fit, 100)
cv_df = 
  cv_df %>% 
  mutate(fit = map(train, ~lm(bwt ~ babysex + gaweeks + bhead * mrace + blength * smoken + wtgain, data = birthweight_fit)),
         model1 = map(train, ~lm(bwt ~ blength + gaweeks, data = birthweight_fit)),
         model2 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = birthweight_fit))) %>% 
  mutate(rmse_fit = map2_dbl(fit, test, ~rmse(model = .x, data = .y)),
         rmse_model1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
         rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)))

# plot of cross-validated prediction error
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + ggtitle("Cross-validated prediction error for different models")
```

It can be seen in the first residual plot that the residuals are randomly distributed near the zero line, except those outliers with extremely low predicted values. The second cross-validated prediction error plot shows that my model 'fit' has a lower prediction error and thus higher predictive accuracy compared to the other two candidate models.

## Problem 2
```{r}
# download the data
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

# use 5000 bootstrap samples 
# fit a linear model with tmax and tmin
# rsq is in glance and betas are in tidy
bootstrap_regression = 
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    tidied = map(models, broom::tidy),
    glanced = map(models, glance)) %>% 
  select(-strap, -models) 

# estimate[1] = intercept, estimate[2] = slope
bootstrap_beta = 
  bootstrap_regression %>% 
  select(-glanced) %>% 
  unnest(tidied) %>% 
  group_by(.id) %>% 
  # log(beta0*beta1)
  summarize(beta_product = log(estimate[[1]]*estimate[[2]]))
# plot the distribution
bootstrap_beta %>% 
  ggplot(aes(x = beta_product)) + geom_density() + labs(x = "log(beta0_hat*beta1_hat)")
# identify the 2.5% and 97.5% quantiles to provide a 95% CI
quantile(bootstrap_beta$beta_product, 0.025)
quantile(bootstrap_beta$beta_product, 0.975)

bootstrap_rsq = 
  bootstrap_regression %>% 
  select(-tidied) %>% 
  unnest(glanced)
# plot the distribution
bootstrap_rsq %>% 
  ggplot(aes(x = r.squared)) + geom_density() 
# identify the 2.5% and 97.5% quantiles to provide a 95% CI
quantile(bootstrap_rsq$`r.squared`, 0.025)
quantile(bootstrap_rsq$`r.squared`, 0.975)
```

The distribution of log(beta0_hat*beta1_hat) follows a normal distribution with the 95% confidence interval of `r quantile(pull(bootstrap_beta, beta_product), 0.025)` and `r quantile(pull(bootstrap_beta, beta_product), 0.975)`. The distribution of r_hat_squared follows a normal distribution with the 95% confidence interval of `r quantile(pull(bootstrap_rsq, r.squared), 0.025)` and `r quantile(pull(bootstrap_rsq, r.squared), 0.975)`.
